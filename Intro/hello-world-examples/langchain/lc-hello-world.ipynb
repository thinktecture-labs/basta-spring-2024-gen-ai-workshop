{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Hello-World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.0.2.post1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain-openai)\n",
      "  Using cached langchain_core-0.1.12-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting numpy<2,>=1 (from langchain-openai)\n",
      "  Using cached numpy-1.26.3-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting openai<2.0.0,>=1.6.1 (from langchain-openai)\n",
      "  Using cached openai-1.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain-openai)\n",
      "  Using cached tiktoken-0.5.2-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.63 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached langsmith-0.0.83-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\sebastiangingter\\miniconda3\\envs\\lc-hello-world\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (23.2)\n",
      "Collecting pydantic<3,>=1 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting requests<3,>=2 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sebastiangingter\\miniconda3\\envs\\lc-hello-world\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.9.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.6.0,>=0.5.2->langchain-openai)\n",
      "  Using cached regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1->langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached pydantic_core-2.14.6-cp312-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sebastiangingter\\miniconda3\\envs\\lc-hello-world\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.6.1->langchain-openai) (0.4.6)\n",
      "Using cached langchain_openai-0.0.2.post1-py3-none-any.whl (28 kB)\n",
      "Using cached langchain_core-0.1.12-py3-none-any.whl (218 kB)\n",
      "Using cached numpy-1.26.3-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached openai-1.8.0-py3-none-any.whl (222 kB)\n",
      "Using cached tiktoken-0.5.2-cp312-cp312-win_amd64.whl (785 kB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
      "Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.6-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Using cached regex-2023.12.25-cp312-cp312-win_amd64.whl (268 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: urllib3, tqdm, tenacity, sniffio, regex, PyYAML, pydantic-core, numpy, jsonpointer, idna, h11, distro, charset-normalizer, certifi, annotated-types, requests, pydantic, jsonpatch, httpcore, anyio, tiktoken, langsmith, httpx, openai, langchain-core, langchain-openai\n",
      "Successfully installed PyYAML-6.0.1 annotated-types-0.6.0 anyio-4.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 distro-1.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 jsonpatch-1.33 jsonpointer-2.4 langchain-core-0.1.12 langchain-openai-0.0.2.post1 langsmith-0.0.83 numpy-1.26.3 openai-1.8.0 pydantic-2.5.3 pydantic-core-2.14.6 regex-2023.12.25 requests-2.31.0 sniffio-1.3.0 tenacity-8.2.3 tiktoken-0.5.2 tqdm-4.66.1 urllib3-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages via pip\n",
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke first completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ladies and gentlemen, distinguished guests, and fellow AI enthusiasts, a warm welcome to the AI workshop at BASTA 2024 Spring! It is an absolute pleasure to have you all here with us today. We are excited to dive into the fascinating world of artificial intelligence and explore its limitless possibilities. Whether you are a seasoned expert or new to the field, we promise an engaging and insightful session that will leave you inspired. So, let's join forces and embark on this incredible journey together. Welcome to the AI workshop!\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"We are live on a conference stage of BASTA 2024 Spring in an AI workshop. Please greet the audience.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, I have a question. The speaker mentioned that AI can be used in various industries. Could you provide some examples of how AI is currently being used in different fields?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Step 1: Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a software developer and you are attending a conference. You are in a workshop about AI. The speaker is talking about AI and you are interested in the topic.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Step 2: we use the LLM already defined\n",
    "\n",
    "# Step 3: We parse the output\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Build the actual chain\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Define the input:\n",
    "input = { \"input\": \"Do you have any questions?\" }\n",
    "\n",
    "# Call the chain\n",
    "chain.invoke(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
